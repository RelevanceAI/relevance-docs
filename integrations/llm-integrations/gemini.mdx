---
title: "Google's Gemini LLM models"
sidebarTitle: "Gemini"
description: "Learn more about Google's Gemini LLM models and how to access them via Google AI Studio and Google Cloud Platform"
---

Google's Gemini LLM models are known for strong coding ability and task complexity handling (especially Google Gemini 2.5 models).

<AccordionGroup>
  <Accordion title="What are they good for?">
    Software development agents, complex task execution.
  </Accordion>
  <Accordion title="How much do they cost?">
    Credits charged per 1,000 tokens processed.
  </Accordion>
  <Accordion title="What native files can they process?">
    Gemini models can process each of the following files:
    - .pdf 
    - .jpg
    - .jpeg
    - .png
    - .webp
    - .gif
    - .wav
    - .mp3
    - .aac
    - .ogg
    - .aiff
    - .flac
    - .mp4
    - .mpeg
    - .mov
    - .avi
    - .flv
    - .mpg
    - .webm
    - .wmv
  </Accordion>
</AccordionGroup>

## Understanding Pricing & Implicit Caching

Gemini models include an automatic **context caching** feature that significantly reduces costs for repeated content. This section explains how it works and why your actual costs may be lower than expected.

### What is Context Caching?

When you make sequential calls to Gemini models (like in multi-turn agent conversations), Gemini automatically caches repeated input tokens and charges them at a much lower rate. This happens automatically—no code changes required.

**What gets cached:**
- Agent instructions (system prompts)
- Tool definitions
- Any repeated context in the prompt prefix
- Previous conversation context

**Cost savings:**
- Cached input tokens are charged at approximately **10x lower rates** than uncached tokens
- This can reduce your overall costs by 50-90% in multi-turn conversations
- The caching works automatically across sequential calls within a short timeframe

### Pricing Example: Gemini 1.5 Pro

Here's how the pricing breaks down for Gemini 1.5 Pro:

| Token Type | Cost per 1,000 tokens | Example |
|------------|----------------------|---------|
| **Uncached input tokens** | 0.315 credits | First time the model sees your agent instructions |
| **Cached input tokens** | 0.0315 credits | Subsequent calls reusing the same instructions |
| **Output tokens** | 1.26 credits | All generated responses |

<Note>
Cached tokens cost **90% less** than uncached tokens, dramatically reducing costs for agents that make multiple LLM calls.
</Note>

### Worked Example: Multi-Turn Agent Conversation

Let's walk through a real example to see how context caching reduces costs:

<Steps>
  <Step title="First Agent Call">
    Your agent makes its first LLM call with:
    - 19,000 input tokens (agent instructions + tool definitions + user query)
    - All tokens are **uncached** on the first call
    - 2,000 output tokens generated
    
    **Cost calculation:**
    - Input: 19,000 tokens × 0.315 credits / 1,000 = **5.99 credits**
    - Output: 2,000 tokens × 1.26 credits / 1,000 = **2.52 credits**
    - **Total: 8.51 credits**
  </Step>
  
  <Step title="Second Agent Call">
    Your agent makes a follow-up call with:
    - 6,000 new input tokens (new user query + context)
    - 13,000 cached tokens (agent instructions + tool definitions from first call)
    - 1,500 output tokens generated
    
    **Cost calculation:**
    - Uncached input: 6,000 tokens × 0.315 credits / 1,000 = **1.89 credits**
    - Cached input: 13,000 tokens × 0.0315 credits / 1,000 = **0.41 credits**
    - Output: 1,500 tokens × 1.26 credits / 1,000 = **1.89 credits**
    - **Total: 4.19 credits**
  </Step>
  
  <Step title="Total Savings">
    **Without caching**, the second call would have cost:
    - 19,000 tokens × 0.315 credits / 1,000 = 5.99 credits (input)
    - 1,500 tokens × 1.26 credits / 1,000 = 1.89 credits (output)
    - Total: 7.88 credits
    
    **With caching**, the second call costs: 4.19 credits
    
    **Savings: 3.69 credits (47% reduction on this call)**
  </Step>
</Steps>

### Understanding the UI Display

You may notice that the credit breakdown in the Relevance AI task details shows different numbers than you expect. Here's why:

<Callout icon="info-circle">
**Important:** The task credit breakdown currently displays **uncached input tokens only**. Cached tokens are charged at the lower cached rate but are not separately displayed in the UI breakdown.
</Callout>

**What this means:**
- If you see "6,000 input tokens" in the UI, there may be an additional 13,000 cached tokens charged at the lower rate
- The **total cost shown is accurate**—it includes both uncached and cached token charges
- When doing manual calculations, you might expect higher costs because you're calculating all tokens at the uncached rate
- **Your actual costs are lower** because of the automatic caching discount

**Example of what you might see:**
```
Task Credit Breakdown:
- Input tokens: 6,000 (shown in UI)
- Output tokens: 1,500
- Total cost: 4.19 credits

Behind the scenes:
- Uncached input: 6,000 tokens @ 0.315 credits/1k = 1.89 credits
- Cached input: 13,000 tokens @ 0.0315 credits/1k = 0.41 credits (not shown separately)
- Output: 1,500 tokens @ 1.26 credits/1k = 1.89 credits
- Total: 4.19 credits ✓
```

### Pricing Transparency

<Tip>
**No markup policy:** Relevance AI passes through Google's exact pricing, including all caching discounts. We do not add any markup to Vendor Credits.
</Tip>

**Key points:**
- Context caching is a Google Gemini feature, not a Relevance AI pricing decision
- This feature **benefits you** with significantly lower costs on multi-turn conversations
- We pass through the exact vendor pricing, including the 90% discount on cached tokens
- Your actual costs will typically be **lower** than advertised base rates due to automatic caching

### Learn More

For complete technical details about Gemini's context caching implementation, see:
- [Gemini Context Caching Documentation](https://ai.google.dev/gemini-api/docs/caching)

<Note>
If you connect your own Google AI Studio or Google Cloud Platform credentials, you'll see the same caching behavior and cost savings directly in your Google billing.
</Note>

## Accessing Gemini models using Google AI Studio

If you use Google AI Studio, you can add your API key to Relevance AI, which will allow you to use your Google AI Studio credits as opposed to your Relevance AI credits. 

You can add your Google AI Studio API key to Relevance AI by following these steps:
1. Generate your API key at [Google AI Studio](https://aistudio.google.com/welcome)
2. Log into Relevance AI and head to 'Integrations & API Keys'
3. Search for 'Google AI Studio Gemini API Key' and add your API key

Once you've successfully added your API key, you will not be charged Relevance AI credits when you use Google Gemini models in your LLM Tool steps and Agents. 

## Accessing Gemini models using Vertex AI on Google Cloud Platform

If you use Google Cloud Platform, you can connect your Vertex AI API key to Relevance AI, which will allow you to use your Google Cloud Platform credits as opposed to your Relevance AI credits. 

You can do this by following these steps:

<Steps>
  <Step title="Enable your Vertex AI API key">
    In Google Cloud Console, enable the Vertex AI API by following the instructions [here](https://cloud.google.com/vertex-ai/docs/featurestore/setup#configure_project).
  </Step>
  <Step title="Create a service account">
    Create a service account in the Google Cloud Console following the instructions at https://cloud.google.com/iam/docs/service-accounts-create. **Make note of the project ID used to create the account.** The service account can have any name and description desired.
    ![Screenshot of creating a Gemini service account](/images/gemini_service_account.png)
  </Step>
  <Step title="Add IAM role">
    Give the service account the Vertex AI Service Agent IAM role. This means the service account is permitted to use Vertex AI and call LLMs in Google Cloud.
  </Step>
  <Step title="Create private key">
    Create a JSON private key for the service account, and download the key file.
  </Step>
  <Step title="Extract the client_email and private_key fields">
    In the key file, extract the client_email and private_key fields exactly as they are written, removing the quotation marks.

    The client_email should be an email address that normally ends with iam.gserviceaccount.com.

    The private_key should be formatted as follows:
    ```shell
    -----BEGIN PRIVATE KEY-----\\nLOTS OF LETTERS AND NUMBERS HERE\\n-----END PRIVATE KEY-----\\n


    ```
  </Step>
  <Step title="Add details to Relevance AI">
    Log into Relevance AI and head to 'Integrations & API Keys'. Search for Google Cloud Platform, and add the Client Email and Private Key, and the Project ID from earlier in step 2.
  </Step>
  <Step title="(Optional) Enter the region">
    Optionally, if the Gemini models are deployed into a particular Google Cloud Region, enter the region. Otherwise, leave it blank. If you enter a region that isn't deployed properly, calling models will fail. See https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations for a list of regions and what the string should look like.
  </Step>
</Steps>

Once your Google Cloud Platform details are added correctly, you will not be charged Relevance AI credits for Google Gemini models.
