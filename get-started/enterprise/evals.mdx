---
title: 'Agent Evals'
sidebarTitle: 'Evals'
description: 'Test, evaluate, and monitor your AI agents with scenario-based evaluations and automated judges'
---

<Info>
**Rollout Status**: Agent Evals is currently being rolled out progressively, starting with Enterprise customers. If you don't see this feature in your account yet, it will be available soon. [Contact support](/get-started/support) if you have questions about access.
</Info>

The Evals tab is your command center for testing and monitoring AI agent performance. Located next to the **Run** tab in the agent builder, Evals enables you to create test scenarios, define evaluation criteria (Judges), run automated evaluations, and analyze results—all without manual testing.

<Frame>
  <img 
    src="/images/enterprise/analytics.png" 
    alt="Evals tab showing Tests, Judges, Runs, and Analytics sections"
  />
</Frame>

## What You Can Do with Evals

<CardGroup cols={2}>
  <Card title="Conduct Tests" icon="flask-vial">
    Create test scenarios that simulate real user interactions. Combine scenarios with Judges to measure accuracy and evaluate agent performance automatically.
  </Card>
  <Card title="Create Judges" icon="scale-balanced">
    Define evaluation criteria that automatically assess agent responses. Judges look for specific conditions and score conversations based on your defined rules.
  </Card>
  <Card title="View Test History" icon="clock-rotate-left">
    Access your complete evaluation run history. Review past test results, compare performance across runs, and track improvements over time.
  </Card>
  <Card title="Analyze Performance" icon="chart-line">
    View agent-level analytics including tasks run, tasks to review, actions used, credits consumed, and escalations over time.
  </Card>
</CardGroup>

---

## Evals Tab Sections

The Evals tab contains five main sections, accessible from the left sidebar:

- **Tests** — Create and manage test scenarios for your agent. Each test can contain multiple scenarios with different prompts and evaluation criteria.
- **Judges** — Configure evaluation criteria that apply across all test runs for the agent. Judges defined here automatically evaluate every test scenario you run.
- **Runs** — View your evaluation run history and results. See average scores, number of conversations evaluated, progress status, and creation dates for all past runs.
- **Analytics** — Monitor agent performance with detailed metrics including tasks run, tasks to review, actions used, credits used, and escalations over time. You can also view model breakdowns (which AI models your agent uses) and action breakdowns (which tools have been used).
- **Errors** — Monitor and troubleshoot issues with error analytics including tasks with errors, unique errors, tool errors, and agent errors. All analytics can be filtered by custom date ranges.

---

## Tests vs. Evaluating Real Tasks

There are two distinct ways to evaluate your agent's performance:

<CardGroup cols={2}>
  <Card title="Tests" icon="flask-vial">
    **Simulated conversations before real interactions**
    
    Tests run sample conversations with your agent using simulated user personas you define. These are synthetic interactions designed to verify your agent behaves correctly before it handles real tasks.
  </Card>
  <Card title="Evaluating Real Tasks" icon="clipboard-check">
    **Assess real conversations that already occurred**
    
    Run evals on tasks your agent has completed with actual users. This lets you measure quality and identify issues in real production conversations.
  </Card>
</CardGroup>

---

## Understanding Judges

Judges are evaluation criteria that automatically assess agent conversations. There are two types of judges, each used in different contexts:

### Scenario Judges

Scenario judges are created within individual test scenarios. They evaluate the specific conversation generated by that scenario's prompt.

- Created inside a test scenario using the **+ Add judge** button
- Only apply to the scenario they're defined in
- Used when running test scenarios from the Tests tab

### Agent-Level Judges

Agent-level judges are standalone evaluators configured in the **Judges** tab. They're used when evaluating real tasks your agent has completed.

- Created in the **Judges** tab (separate from tests)
- Apply to any conversation you select and run an eval on from the Run tab
- Useful for universal criteria like professional tone, no hallucinations, or brand voice compliance

To create an agent-level judge:

<div style={{ width:"100%",position:"relative","padding-top":"56.75%" }}>
<iframe src="https://app.supademo.com/embed/PLACEHOLDER_AGENT_LEVEL_JUDGE" frameBorder="0" title="Creating an agent-level judge" allow="clipboard-write; fullscreen" webkitAllowFullscreen="true" mozAllowFullscreen="true" allowFullscreen style={{ position:"absolute",top:0,left:0,width:"100%",height:"100%",border:"3px solid #5E43CE",borderRadius:"10px" }} />
</div>

1. Go to the **Judges** tab in the Evals section
2. Click **+ New Judge**
3. Enter a **Name** for the judge (e.g., "Professional Tone")
4. Enter an **Evaluation rule** describing the criteria for passing
5. Save the judge

<Note>
Currently, you cannot take an existing agent-level judge and add it to a test scenario. These two types of judges are managed separately.
</Note>

---

## Creating a Test

<div style={{ width:"100%",position:"relative","padding-top":"56.75%" }}>
<iframe src="https://app.supademo.com/embed/PLACEHOLDER_CREATING_TEST" frameBorder="0" title="Creating a test" allow="clipboard-write; fullscreen" webkitAllowFullscreen="true" mozAllowFullscreen="true" allowFullscreen style={{ position:"absolute",top:0,left:0,width:"100%",height:"100%",border:"3px solid #5E43CE",borderRadius:"10px" }} />
</div>

Follow these steps to create your first evaluation test:

<Steps>
  <Step title="Navigate to Tests">
    Open your agent in the builder and click the **Evals** tab (next to the Run tab). Select **Tests** from the left sidebar.
  </Step>
  
  <Step title="Create a New Test">
    Click the **+ New Test** button. Enter a name for your test and click **Create**.
  </Step>
  
  <Step title="Open Your Test">
    Click on the test you just created to open it.
  </Step>
  
  <Step title="Add a Test Scenario">
    Click the **+ New test scenario** button to create a scenario within your test.
  </Step>
  
  <Step title="Configure the Scenario">
    Fill in the scenario details:
    
    | Field | Description | Example |
    |-------|-------------|---------|
    | **Scenario name** | A descriptive name for this test case | "Response Empathy" |
    | **Scenario prompt** | The persona or situation the simulated user will adopt | "You are a long-time customer who was recently charged twice for the same order. You've already contacted support once without resolution and are feeling frustrated but willing to give the agent a chance to help." |
    | **Max turns** | Maximum conversation turns (1-50) | 10 |
  </Step>
  
  <Step title="Add Scenario Judges">
    Add scenario judges to define how this specific scenario should be evaluated:
    
    | Field | Description | Example |
    |-------|-------------|---------|
    | **Judge name** | Name of the evaluation criterion | "Empathy Shown" |
    | **Evaluation rule** | Detailed criteria for passing/failing | "Did the agent acknowledge the customer's frustration and express empathy before offering solutions? The response should show understanding of the emotional state and validate their concerns." |
    
    Click **+ Add judge** to add more evaluation criteria to the scenario.
  </Step>
  
  <Step title="Save the Scenario">
    Click **Update test scenario** to save your configuration.
  </Step>
</Steps>

<Tip>
You can add multiple scenarios to a single test to evaluate different aspects of your agent's behavior. Each scenario can have its own prompt, max turns, and judges.
</Tip>

### Example Scenarios

Here are some example test scenarios you might create:

<AccordionGroup>
  <Accordion title="Customer Support - Empathy Test">
    **Scenario name**: Response Empathy
    
    **Scenario prompt**: You are a long-time customer who was recently charged twice for the same order. You've already contacted support once without resolution and are feeling frustrated but willing to give the agent a chance to help. Express your concerns clearly and see if the agent acknowledges your situation before jumping to solutions.
    
    **Max turns**: 10
    
    **Judge**: Empathy Shown
    - *Evaluation rule*: Did the agent acknowledge the customer's frustration and express empathy before offering solutions? The response should show understanding of the emotional state and validate their concerns.
  </Accordion>
  
  <Accordion title="Sales - Product Knowledge Test">
    **Scenario name**: Product Expertise
    
    **Scenario prompt**: You are a procurement manager at a mid-sized company evaluating solutions for your team. You need specific details about enterprise pricing tiers, integration capabilities with existing tools like Salesforce and HubSpot, and data security certifications. Ask clarifying questions and compare features against competitors you're also considering.
    
    **Max turns**: 15
    
    **Judge**: Accurate Information
    - *Evaluation rule*: Did the agent provide accurate product information without making claims that cannot be verified? Responses should be factual, reference actual product capabilities, and acknowledge when information needs to be confirmed by a sales representative.
  </Accordion>
  
  <Accordion title="Support - Escalation Handling">
    **Scenario name**: Escalation Request
    
    **Scenario prompt**: You are a paying customer who has experienced a service outage affecting your business operations. You've already troubleshooted with the knowledge base articles and need to speak with a senior support engineer or account manager. Be firm but professional in your request, and provide context about the business impact.
    
    **Max turns**: 5
    
    **Judge**: Appropriate Escalation
    - *Evaluation rule*: Did the agent acknowledge the severity of the situation, validate the customer's need for escalation, and initiate a handoff to a human representative while maintaining a professional and empathetic tone throughout?
  </Accordion>
</AccordionGroup>

---

## Running Evaluations

There are two ways to run evaluations: test scenarios use simulated conversations to verify agent behavior before real interactions, while evals on real tasks assess actual conversations that have already occurred with users.

### Running a Test Scenario

<div style={{ width:"100%",position:"relative","padding-top":"56.75%" }}>
<iframe src="https://app.supademo.com/embed/PLACEHOLDER_RUNNING_TEST_SCENARIO" frameBorder="0" title="Running a test scenario" allow="clipboard-write; fullscreen" webkitAllowFullscreen="true" mozAllowFullscreen="true" allowFullscreen style={{ position:"absolute",top:0,left:0,width:"100%",height:"100%",border:"3px solid #5E43CE",borderRadius:"10px" }} />
</div>

Test scenarios simulate conversations with your agent using the personas you've defined. This uses **scenario judges** to evaluate each simulated conversation:

<Steps>
  <Step title="Select Scenarios">
    From the Tests tab, select the test scenarios you want to include in the evaluation run.
  </Step>
  
  <Step title="Click Run">
    Click the **Run** button to start the evaluation.
  </Step>
  
  <Step title="Name Your Run">
    Enter a name for the evaluation run (e.g., "Scenario Run - Jan 14, 12:14 PM"). A default name with timestamp is provided.
  </Step>
  
  <Step title="Start Evaluation">
    Click **Run** to begin. The system will simulate conversations with your agent based on your scenario prompts. The scenario judges you defined will evaluate each conversation.
  </Step>
</Steps>

### Running an Eval on Real Tasks

<div style={{ width:"100%",position:"relative","padding-top":"56.75%" }}>
<iframe src="https://app.supademo.com/embed/PLACEHOLDER_RUNNING_EVAL_REAL_TASKS" frameBorder="0" title="Running an eval on real tasks" allow="clipboard-write; fullscreen" webkitAllowFullscreen="true" mozAllowFullscreen="true" allowFullscreen style={{ position:"absolute",top:0,left:0,width:"100%",height:"100%",border:"3px solid #5E43CE",borderRadius:"10px" }} />
</div>

Evaluate real tasks your agent has completed with actual users. This uses your **agent-level judges** to assess production conversations:

<Steps>
  <Step title="Go to the Run Tab">
    Navigate to the **Run** tab in your agent builder to view your task history.
  </Step>
  
  <Step title="Select Tasks">
    Click the checkbox next to one or more tasks you want to evaluate. You can select multiple tasks or click "Select all loaded tasks" to evaluate them in bulk.
  </Step>
  
  <Step title="Click Run Eval">
    With tasks selected, click the **Run Eval** button from the action menu.
  </Step>
  
  <Step title="Name and Run">
    Enter a name for the evaluation run and click **Run Eval** to start. The evaluation will run against all agent-level judges you've configured in the Judges tab.
  </Step>
</Steps>

---

## Understanding Results

After running an evaluation, you'll see a detailed results screen:

### Run Summary

The top of the results page shows key metrics:

| Metric | Description |
|--------|-------------|
| **Average Score** | Overall pass rate across all scenarios and judges |
| **Number of Conversations** | How many test conversations were evaluated |
| **Agent Version** | The version of the agent that was tested |

### Scenario Results

Each scenario displays:

| Column | Description |
|--------|-------------|
| **Status** | Running, Completed, or Failed |
| **Name** | The scenario name |
| **Score** | Percentage of judges that passed (shown with progress bar) |
| **Judges** | Pass/fail count (e.g., "1/1 passed") |
| **Credits** | Credits consumed for this scenario |

### Viewing Conversation Details

Click **View Conversation** on any scenario to see:

1. **The full conversation** between the simulated user and your agent
2. **Judge results** with detailed explanations of why each judge passed or failed

For example, an "Empathy Shown" judge might show:
> **Pass**: The agent demonstrated strong empathy throughout the conversation. Key examples include: acknowledging the customer's frustration with being transferred multiple times ("I completely understand how upsetting it must be to feel like you're not getting the help you need"), validating her experience with the double charge ("I truly understand how frustrating it is to be charged twice"), and directly addressing her skepticism by saying "I completely understand your concerns, especially given your previous experience."

---

## Runs History

Access your complete evaluation history from the **Runs** section. This shows:

| Column | Description |
|--------|-------------|
| **Run name** | Name of the evaluation run |
| **Average score** | Overall pass rate with visual progress bar |
| **# Conversations** | Number of conversations in the run |
| **Progress** | Completion status (e.g., "1/1") |
| **Date created** | When the run was executed |

Click on any past run to view its detailed results.

---

## Best Practices

<CardGroup cols={2}>
  <Card title="Start Simple" icon="seedling">
    Begin with a few core scenarios that test your agent's primary use cases. Add complexity as you learn what matters most.
  </Card>
  <Card title="Be Specific with Judges" icon="bullseye">
    Write detailed evaluation rules. Vague criteria lead to inconsistent results. Include specific examples of what passing looks like.
  </Card>
  <Card title="Test Edge Cases" icon="triangle-exclamation">
    Create scenarios for difficult situations: angry customers, off-topic requests, requests to bypass rules, etc.
  </Card>
  <Card title="Run Regularly" icon="arrows-rotate">
    Evaluate your agent after making changes to prompts, tools, or knowledge. Use runs history to track improvements over time.
  </Card>
</CardGroup>

---

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="How many scenarios can I have in a test?">
    You can add as many scenarios as needed to a single test. Each scenario is evaluated independently and can have its own judges.
  </Accordion>
  
  <Accordion title="How are credits calculated for evaluations?">
    Credits are consumed based on the conversation length and the models used during the evaluation. Each scenario shows its credit usage in the results.
  </Accordion>
  
  <Accordion title="Can I rerun a previous evaluation?">
    Yes, you can run the same test scenarios again at any time. Each run is saved in your Runs history, allowing you to compare results across different agent versions.
  </Accordion>
  
  <Accordion title="What's the difference between scenario judges and agent-level judges?">
    Scenario judges are created within test scenarios and only evaluate conversations generated by that scenario. Agent-level judges are created in the Judges tab and are used when evaluating previous tasks from the Run tab.
  </Accordion>
  
  <Accordion title="I don't see the Evals tab. How do I get access?">
    Agent Evals is being rolled out progressively, starting with Enterprise customers. If you don't have access yet, it will be available soon. [Contact support](/get-started/support) if you need immediate access or have questions.
  </Accordion>
</AccordionGroup>


